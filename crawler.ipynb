{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫\n",
    "\n",
    "## 什么是爬虫\n",
    "\n",
    "不考虑复杂的内容，我们可以简单的将“上网”概括为“浏览器向服务器按照用户意愿发送一系列请求，并将服务器的响应呈现给用户“的过程。而爬虫便是一种可以模拟网页浏览器向网站发送、处理请求的自动化脚本，帮助我们获得服务器的数据，进而用于其他需求。\n",
    "\n",
    "那么爬虫可以做什么呢？可以认为一切与网络请求相关的内容爬虫都可以按照我们的需要进行处理。\n",
    "\n",
    "## 课前准备\n",
    "\n",
    "### 预备知识\n",
    "\n",
    "我们期待您已经掌握了 Python 的使用，了解了 Web 的基本知识，如果您对这两方面有疑惑，可以参考：\n",
    "- [2023 科协暑培 Web 部分](https://summer23.net9.org/basic/web/)\n",
    "- [2023 科协暑培 Python 部分](https://summer23.net9.org/basic/python/)\n",
    "\n",
    "HTTP 基本知识：\n",
    "- HTTP 入门: [https://www.ruanyifeng.com/blog/2016/08/http.html](https://www.ruanyifeng.com/blog/2016/08/http.html)\n",
    "- HTTP 方法: [https://www.w3schools.com/tags/ref_httpmethods.asp](https://www.w3schools.com/tags/ref_httpmethods.asp)\n",
    "- HTTP Status Code: [https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status)\n",
    "- HTTP Cookie: [https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Cookies](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Cookies)\n",
    "\n",
    "Route yourself in XML:\n",
    "- XPATH: [https://www.w3schools.com/xml/xpath_intro.asp](https://www.w3schools.com/xml/xpath_intro.asp)\n",
    "\n",
    "### 环境准备\n",
    "\n",
    "```shell\n",
    "conda create -n crawler python=3.10\n",
    "conda activate crawler\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何构建爬虫\n",
    "\n",
    "### 网络请求分析\n",
    "\n",
    "构建爬虫的关键在于分析服务器请求，分析我们期望获得的数据在哪个或哪些请求中，进而构建“API Chains”。\n",
    "\n",
    "网络请求的响应往往有三种类型：\n",
    "- Json\n",
    "- HTML\n",
    "- 二进制文件\n",
    "\n",
    "对于 Json，我们直接使用 `Json` 分析即可；对于 HTML，我们则需要使用 `BeautifulSoup4` 搭配 HTML 解析器进行分析；对于二进制文件，我们可以直接保存。\n",
    "\n",
    "之后我们将通过对 [知乎](https://www.zhihu.com) 的爬取实践来熟悉构建爬虫的整个流程。\n",
    "\n",
    "### 爬取知乎首页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zhihu_hot_urls() -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the hot urls list of zhihu\n",
    "\n",
    "    Returns:\n",
    "        The hot urls list of zhihu\n",
    "    \"\"\"\n",
    "    url = \"https://www.zhihu.com/hot\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15\",\n",
    "    }\n",
    "    with open(\"config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    headers[\"cookie\"] = config[\"cookie\"]\n",
    "    resp = requests.get(url=url, headers=headers)\n",
    "    soup = BS(resp.text, \"lxml\")\n",
    "    hot_list = []\n",
    "    for item in soup.find_all(\"div\", class_=\"HotItem-content\"):\n",
    "        hot_list.append(item.find(\"a\")[\"href\"])\n",
    "    return hot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_urls = get_zhihu_hot_urls()\n",
    "hot_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取某项问题下的所有回答\n",
    "\n",
    "#### API 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_answers(qid: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get all answers of a question\n",
    "\n",
    "    Args:\n",
    "        qid: The id of the question\n",
    "\n",
    "    Returns:\n",
    "        The list of all answers\n",
    "    \"\"\"\n",
    "    url = f\"https://www.zhihu.com/api/v4/questions/{qid}/feeds?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cattachment%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Cis_labeled%2Cpaid_info%2Cpaid_info_content%2Creaction_instruction%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cvip_info%2Cbadge%5B%2A%5D.topics%3Bdata%5B%2A%5D.settings.table_of_content.enabled&limit=5&offset=0&order=default&platform=desktop&session_id=1690710830265112023\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15\",\n",
    "    }\n",
    "    with open(\"config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    headers[\"cookie\"] = config[\"cookie\"]\n",
    "    answers = []\n",
    "    while True:\n",
    "        resp = requests.get(url=url, headers=headers)\n",
    "        data = resp.json()[\"data\"]\n",
    "        answers.extend(data)\n",
    "        if resp.json()[\"paging\"][\"is_end\"]:\n",
    "            break\n",
    "        url = resp.json()[\"paging\"][\"next\"]\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_all_answers(\"613681273\")\n",
    "authors = [\n",
    "    d[\"target\"][\"author\"][\"name\"] for d in data if d[\"target\"][\"type\"] == \"answer\"\n",
    "]\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selenium + WebDriver 模拟浏览器操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait as wdw\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_opetions = ChromeOptions()\n",
    "chrome_opetions.binary_location = \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\"\n",
    "driver = selenium.webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_opetions)\n",
    "driver.get(f\"https://www.zhihu.com/question/613681273\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试输入账号密码登录功能\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "username = driver.find_element(By.XPATH, '/html/body/div[5]/div/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div/div[1]/div/form/div[2]/div[2]/label/input')\n",
    "username.send_keys(config[\"username\"])\n",
    "\n",
    "password = driver.find_element(By.XPATH, '/html/body/div[5]/div/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div/div[1]/div/form/div[3]/div/label')\n",
    "password.send_keys(config[\"password\"])\n",
    "\n",
    "login = driver.find_element(By.XPATH, '/html/body/div[5]/div/div/div/div[2]/div/div/div/div[2]/div[1]/div[2]/div/div[1]/div/form/button')\n",
    "login.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关闭窗口\n",
    "wdw(driver, 10).until(EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[2]/button')))\n",
    "button_quit = driver.find_element(By.XPATH, '/html/body/div[5]/div/div/div/div[2]/button')\n",
    "button_quit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    driver.execute_script(\"window.scrollBy(0, -10);\")\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    if driver.find_elements(By.XPATH, '//*[@id=\"root\"]/div/main/div/div/div[3]/div[1]/div/div[2]/a/button'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = [item.text for item in driver.find_elements(By.CLASS_NAME, 'UserLink-link')]\n",
    "authors = [item for item in authors if item != '']\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除去上述内容外，`Selenium` 还有很多有用的方法，包含但不限于：\n",
    "- 等待到某个元素加载完成；\n",
    "- Action 链；\n",
    "- 键盘组合键；\n",
    "- 几乎一切你在使用浏览器中会用到的操作；\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy 入门\n",
    "\n",
    "> Scrapy (/ˈskreɪpaɪ/) is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\n",
    "> \n",
    "> Even though Scrapy was originally designed for web scraping, it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler.\n",
    "\n",
    "我们在之前使用的 BeautifulSoup4，lxml 等都只是构建爬虫过程中使用的工具，而 Scrapy 则是构建爬虫的框架。Scrapy 之于爬虫就如同 Django 之于后端，React 之于前端，其为构建爬虫预先准备了大量丰富的方法。\n",
    "\n",
    "Scrapy 的优越性太多了，可以直接参考 [官网](https://docs.scrapy.org/en/latest/intro/overview.html) 的描述。\n",
    "\n",
    "### Architecture & Components\n",
    "\n",
    "Scrapy 的架构如下：\n",
    "\n",
    "![](images/scrapy_architecture.png)\n",
    "\n",
    "框架的组件包括：\n",
    "1. Scrapy Engine: 调节其余各组件，控制数据流；\n",
    "2. Schedule: 优先队列，调度 requests；\n",
    "3. Downloader: 下载器，即向网络发送请求；\n",
    "4. Spider: 自定义类，主类，解析 responses，构建 items 等；\n",
    "5. Item Pipeline: 在 Spider 生成 Item 后，在该组件中执行后续工作，例如验证，加入数据库等；\n",
    "6. Downloader middlewares: 中间件；\n",
    "7. Spider middlewares: 中间件\n",
    "\n",
    "### A Simple Example\n",
    "\n",
    "让我们用 Scrapy 框架来实现知乎爬虫：\n",
    "\n",
    "```shell\n",
    "conda activate crawler\n",
    "scrapy startproject zhihu\n",
    "cd zhihu\n",
    "scrapy genspider hotpage zhihu.com/hot\n",
    "```\n",
    "\n",
    "之后的构建就可以听回放喽！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "**ddl：2023.08.15**\n",
    "\n",
    "当你遇到困难时，**请随时在群中询问**，你也可以参考 [](https://github.com/KMing-L/crawler) 中的实现。\n",
    "\n",
    "只需在以下任务选择列表中选择任意一个任务的基本功能完成即可获得满分。\n",
    "\n",
    "### 提交方式\n",
    "\n",
    "请将你的代码上传至 [TsinghuaGit](https://git.tsinghua.edu.cn)，要求仓库中必须包含代码的使用说明，请将其写在 `readme.md` 中。之后请将您的姓名+学号+仓库链接提交至 [](https://github.com/sast-summer-training-2023/sast2023-crawler/issues)，如担心隐私泄漏，可通过其他方式告知讲师。\n",
    "\n",
    "### 任务列表\n",
    "\n",
    "选一做即可。\n",
    "\n",
    "#### 1. 清华云盘爬虫\n",
    "\n",
    "Input：某个清华云盘共享文件夹链接，例如 [](https://cloud.tsinghua.edu.cn/d/ba64d0debd0e4ad4bf92/)；\n",
    "\n",
    "Output：下载该文件夹内所有文件，且按照原文件的组织方式组织文件。\n",
    "\n",
    "#### 清华大学教参平台爬虫\n",
    "\n",
    "Input：某本教参详情页的 URL，例如 [](http://reserves.lib.tsinghua.edu.cn/Search/BookDetail?bookId=11ad92b1-d4f1-4ea9-8d2c-310640ba96b1)；\n",
    "\n",
    "Output：包含该本书的所有图片的 PDF。\n",
    "\n",
    "#### Bilibili 网页端视频下载\n",
    "\n",
    "Input：某个视频的 BV 号，例如 `BV1C14y1z7xh`；\n",
    "\n",
    "Output：下载该视频，需要支持下载 1080P(若有)，可以不合并视频&音频。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写在最后\n",
    "\n",
    "当你进行商业用途或可能占用对方服务器大量资源的行为时，请优先查阅 `.../robots.txt` 文件。\n",
    "\n",
    "爬虫只是用来获取数据的途径，而如何解读数据才能体现其核心价值。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
